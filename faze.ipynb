{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Faze** - face & gaze detection for attention prediction and emotion tracking\n",
    "#### Shifeng, May 2020\n",
    "Run the code cells below sequentially. Importing packages might take 10 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "cvpath = \"/opt/ros/kinetic/lib/python2.7/dist-packages\"\n",
    "if cvpath in sys.path: sys.path.remove(cvpath)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util.visualize import plot_all, plot_emotions\n",
    "from util.gaze import vector_to_pitchyaw, pitchyaw_to_vector\n",
    "from util.emotions import get_emotion\n",
    "from run_with_webcam import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize your webcam, '0' is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Detector\n",
    "* Run the cell below to get started.\n",
    "* Two cv2 windows will pop up.\n",
    "    * *Might be hidden behind this program when launched*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel to cm conversion (image plane)\n",
    "p = 7./1200\n",
    "focal_len = 2.7\n",
    "\n",
    "current_face, landmarks, right_eye, left_eye = None, None, None, None\n",
    "smooth = 0.7\n",
    "\n",
    "emotion_history = []\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    \"\"\"\n",
    "    Read from Webcam\n",
    "    \"\"\"\n",
    "    _, frame_bgr = webcam.read()\n",
    "    if frame_bgr is None:\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "    iteration += 1\n",
    "\n",
    "    orig_frame = frame_bgr.copy()\n",
    "    cv2.imshow(\"Webcam\", orig_frame)\n",
    "    frame = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    frame_h,frame_w, _ = frame.shape\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Face detection + temporal smoothing of detection\n",
    "    \"\"\"\n",
    "    # Haar cascade detector\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    if len(faces):\n",
    "        next_face = faces[0]\n",
    "        if current_face is not None:\n",
    "            current_face[:2] = smooth * next_face[:2] + (1 - smooth) * current_face[:2]\n",
    "            current_face[3:] = smooth * next_face[3:] + (1 - smooth) * current_face[3:]\n",
    "        else:\n",
    "            current_face = next_face\n",
    "    \n",
    "    \"\"\"\n",
    "    Advanced Processing\n",
    "    \"\"\"\n",
    "    emotion_frame = []\n",
    "    if current_face is None:\n",
    "        orig_frame = cv2.putText(orig_frame, 'No face detected', (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        draw_cascade_face(current_face, orig_frame)\n",
    "        \n",
    "        \"\"\"\n",
    "        Emotion Detection\n",
    "        \"\"\"\n",
    "        if iteration % 2 == 0:\n",
    "            emotion = get_emotion(current_face, gray)\n",
    "            # emotion smoothing\n",
    "            if len(emotion_history) >= 3:\n",
    "                emotion = emotion*0.5 + emotion_history[-1]*0.3 + emotion_history[-2]*0.2\n",
    "            emotion_history.append(emotion)\n",
    "            if len(emotion_history) > 61:\n",
    "                del emotion_history[0]\n",
    "            emotion_frame = plot_emotions(emotion_history)\n",
    "                \n",
    "            \n",
    "        \"\"\"\n",
    "        Eye Landmark Detection\n",
    "        \"\"\"\n",
    "        next_landmarks = detect_landmarks(current_face, gray)\n",
    "        if landmarks is not None:\n",
    "            landmarks = next_landmarks * smooth + (1 - smooth) * landmarks\n",
    "        else:\n",
    "            landmarks = next_landmarks\n",
    "        draw_landmarks(landmarks, orig_frame)\n",
    "        \n",
    "        \"\"\"\n",
    "        Gaze Prediction\n",
    "        \"\"\"\n",
    "        if landmarks is not None:\n",
    "            eye_samples = segment_eyes(gray, landmarks)\n",
    "            # eyenet inference\n",
    "            eye_preds = run_eyenet(eye_samples)\n",
    "            left_eyes = list(filter(lambda x: x.eye_sample.is_left, eye_preds))\n",
    "            right_eyes = list(filter(lambda x: not x.eye_sample.is_left, eye_preds))\n",
    "\n",
    "            if left_eyes:\n",
    "                left_eye = smooth_eye_landmarks(left_eyes[0], left_eye, smoothing=0.2, gaze_smoothing=0.5)\n",
    "            if right_eyes:\n",
    "                right_eye = smooth_eye_landmarks(right_eyes[0], right_eye, smoothing=0.2, gaze_smoothing=0.5)\n",
    "            \n",
    "            gaze_vectors = np.empty((2,3))\n",
    "            for ep in [left_eye, right_eye]:\n",
    "                for (x, y) in ep.landmarks[16:33]:\n",
    "                    color = (160, 230, 160)\n",
    "#                     if ep.eye_sample.is_left:\n",
    "#                         color = (230, 160, 160)\n",
    "                    cv2.circle(orig_frame,(int(round(x)), int(round(y))), 1, color, -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "                gaze = ep.gaze.copy()\n",
    "                if ep.eye_sample.is_left:\n",
    "                    gaze[1] = -gaze[1]\n",
    "                    gaze_vectors[0,:] = pitchyaw_to_vector(np.expand_dims(gaze,0))\n",
    "                else:\n",
    "                    gaze_vectors[1,:] = pitchyaw_to_vector(np.expand_dims(gaze,0))\n",
    "                util.gaze.draw_gaze(orig_frame, ep.landmarks[-2], gaze, length=60.0, thickness=2)\n",
    "    \n",
    "    \"\"\"\n",
    "    Visualization\n",
    "    \"\"\"\n",
    "    plt_frame = []\n",
    "    if len(faces):\n",
    "        (x,y,w,h) = current_face\n",
    "        face_size = abs(w - h)/4 + min(w,h)\n",
    "        face_distance = 5960/face_size    \n",
    "        face_img_frame = np.array([(x-frame_w/2)*p, (y-frame_h/2)*p, 2.7])\n",
    "        face_world_frame = face_img_frame/np.linalg.norm(face_img_frame) * face_distance\n",
    "        plt_frame = plot_all(face_world_frame, gaze_vectors)\n",
    "    \n",
    "    if len(plt_frame):\n",
    "        cv2.imshow(\"3D Plot\", plt_frame)\n",
    "        \n",
    "    if len(emotion_frame):\n",
    "        cv2.imshow(\"Emotion tracking\", emotion_frame)\n",
    "    \n",
    "    cv2.imshow(\"Webcam\", orig_frame)\n",
    "    \n",
    "    time.sleep(0.04)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release camera and close all cv2 windows at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcam test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame_bgr = webcam.read()\n",
    "    orig_frame = frame_bgr.copy()\n",
    "    cv2.imshow(\"Webcam\", orig_frame)\n",
    "    cv2.waitKey(1)\n",
    "    time.sleep(0.02)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When everything done, release the capture\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Stuff\n",
    "**reference size: 13x13cm**\n",
    "* 34cm: 170\n",
    "* 51cm: 120px\n",
    "* 68cm: 93px\n",
    "* 73cm: 77px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "d = np.array([34,51,57,68,73,92])\n",
    "h = np.array([170,120,110,93,78,62])\n",
    "y = 5960/d\n",
    "# for i in range(-10,11):\n",
    "#     c = (6000 + i*10)\n",
    "#     y = c/d\n",
    "#     error = abs(np.sum(h**2 - y**2))**0.5\n",
    "#     print(c,error)\n",
    "\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,200)\n",
    "plt.plot(d,h)\n",
    "plt.plot(d,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
